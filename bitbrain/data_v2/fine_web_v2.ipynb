{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\".cache/modelscope/datasets/AI-ModelScope/chinese-fineweb-edu-v2\",\n",
    "                       split=\"train\",\n",
    "                       num_proc=40)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'completion': '昭通机场（ZPZT）是位于中国云南昭通的民用机场，始建于1935年，1960年3月开通往返航班“昆明－昭通”，原来属军民合用机场。1986年机场停止使用。1991年11月扩建，于1994年2月恢复通航。是西南地区「文明机场」，通航城市昆明。 机场占地1957亩，飞行区等级为4C，有一条跑道，长2720米，宽48米，可供波音737及以下机型起降。机坪面积6600平方米，停机位2个，航站楼面积1900平方米。位于城东6公里处，民航路与金鹰大道交叉处。\\n航点\\n客服电话\\n昭通机场客服电话：0870-2830004',\n",
       " 'source': 'wikipedia.zh2307'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# Dataset({\n",
    "#     features: ['text', 'score', '__index__', 'source'],\n",
    "#     num_rows: 187668844\n",
    "# })\n",
    "\n",
    "# 1. 定义保存分类后数据集的根目录\n",
    "output_dir = '/DATA/disk2/yuhang/.cache/bit_brain_data/chinese-fineweb-classification'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(f\"\\n分类后的数据集将被保存在 '{output_dir}' 目录下。\\n\")\n",
    "\n",
    "# 2. 获取所有唯一的 'source' 值\n",
    "unique_sources = dataset.unique('source')\n",
    "print(f\"找到的唯一 'source' 值: {unique_sources}\")\n",
    "\n",
    "# 3. 外层循环：遍历每个 'source'\n",
    "for source_value in unique_sources:\n",
    "    print(f\"\\n--- 正在处理 Source: '{source_value}' ---\")\n",
    "    \n",
    "    # 根据 source 创建子目录\n",
    "    source_dir = os.path.join(output_dir, f\"source_{source_value}\")\n",
    "    if not os.path.exists(source_dir):\n",
    "        os.makedirs(source_dir)\n",
    "        \n",
    "    # 过滤出当前 source 的所有数据\n",
    "    # datasets.filter() 非常高效，它不会立即加载数据到内存\n",
    "    source_dataset = dataset.filter(lambda example: example['source'] == source_value,\n",
    "                                    num_proc=40)\n",
    "    \n",
    "    # 4. 定义 score 的划分范围 (bins)\n",
    "    # 根据数据集本身的score分布进行划分\n",
    "    score_bins = [\n",
    "        (0.6, 0.75),\n",
    "        (0.75, 1.0)\n",
    "    ]\n",
    "    print(f\"  将 'score' 按照预设范围进行划分。\")\n",
    "  \n",
    "    # 5. 内层循环：遍历每个 score 范围\n",
    "    for i, (lower_bound, upper_bound) in enumerate(score_bins):\n",
    "        \n",
    "        # 定义一个清晰的目录名来表示范围\n",
    "        range_name = f\"score_{lower_bound}_to_{upper_bound}\"\n",
    "        \n",
    "        # 处理最后一个区间的边界，使其包含上界 (e.g., score == 1.0)\n",
    "        if i == len(score_bins) - 1:\n",
    "            print(f\"  --> 正在处理 Score 范围: [{lower_bound}, {upper_bound}]...\")\n",
    "            # 过滤条件： lower_bound <= score <= upper_bound\n",
    "            final_filtered_dataset = source_dataset.filter(\n",
    "                lambda example: lower_bound <= example['score'] <= upper_bound,\n",
    "                num_proc=40 # 多进程加速\n",
    "            )\n",
    "        else:\n",
    "            print(f\"  --> 正在处理 Score 范围: [{lower_bound}, {upper_bound})...\")\n",
    "            # 过滤条件： lower_bound <= score < upper_bound\n",
    "            final_filtered_dataset = source_dataset.filter(\n",
    "                lambda example: lower_bound <= example['score'] < upper_bound,\n",
    "                num_proc=40 # 多进程加速\n",
    "            )\n",
    "\n",
    "        # 优化：如果过滤后的数据集为空，则跳过保存\n",
    "        if len(final_filtered_dataset) == 0:\n",
    "            print(f\"      范围 '{range_name}' 内没有数据，跳过保存。\")\n",
    "            continue\n",
    "            \n",
    "        # 6. 将最终过滤出的数据集保存到磁盘\n",
    "        save_path = os.path.join(source_dir, range_name)\n",
    "        \n",
    "        # 检查是否已有数据，若有则可以跳过或覆盖\n",
    "        if os.path.exists(save_path):\n",
    "            print(f\"      目录 '{save_path}' 已存在，跳过保存。\")\n",
    "            continue\n",
    "            \n",
    "        final_filtered_dataset.save_to_disk(save_path)\n",
    "        print(f\"      数据已保存到: '{save_path}'，包含 {len(final_filtered_dataset)} 行。\")\n",
    "\n",
    "print(\"\\n--- 所有分类和保存任务已完成！ ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "def get_dir_size(path):\n",
    "    \"\"\"\n",
    "    计算指定路径下所有文件和子文件夹的总大小。\n",
    "    这是一个递归函数，意味着它会调用自己来处理子文件夹。\n",
    "    \"\"\"\n",
    "    total_size = 0\n",
    "    # Path(path).rglob('*') 会遍历目录下的所有内容，包括子文件夹里的\n",
    "    # 这是一种更现代、更简洁的写法\n",
    "    for file in Path(path).rglob('*'):\n",
    "        # 确保我们只计算文件的大小\n",
    "        if file.is_file():\n",
    "            total_size += file.stat().st_size\n",
    "    return total_size\n",
    "\n",
    "def format_size(size_bytes):\n",
    "    \"\"\"\n",
    "    将字节大小格式化为人类易读的形式 (B, KB, MB, GB, TB)。\n",
    "    \"\"\"\n",
    "    if size_bytes == 0:\n",
    "        return \"0B\"\n",
    "    # 定义大小单位的元组\n",
    "    size_names = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\n",
    "    # 使用log计算单位的索引\n",
    "    i = int(math.floor(math.log(size_bytes, 1024)))\n",
    "    # 计算转换后的大小\n",
    "    p = math.pow(1024, i)\n",
    "    s = round(size_bytes / p, 2)\n",
    "    return f\"{s} {size_names[i]}\"\n",
    "\n",
    "# 1. 定义数据集的根目录\n",
    "# 根据你的日志，数据保存在这个路径下\n",
    "root_dir = '/DATA/disk2/yuhang/.cache/bit_brain_data/chinese-fineweb-classification'\n",
    "# 我们只关心高质量数据集\n",
    "target_score_dir_name = 'score_0.75_to_1.0'\n",
    "\n",
    "# 用一个字典来存储每个source和它对应的大小\n",
    "source_sizes = {}\n",
    "\n",
    "print(f\"开始扫描目录: {root_dir}\\n\")\n",
    "\n",
    "# 2. 遍历根目录，找到所有的 source_* 文件夹\n",
    "# os.scandir 比 os.listdir 更高效，因为它在扫描时就获取了文件信息\n",
    "try:\n",
    "    for entry in os.scandir(root_dir):\n",
    "        # 确保它是一个目录并且以 'source_' 开头\n",
    "        if entry.is_dir() and entry.name.startswith('source_'):\n",
    "            # 提取 source 的名字，比如从 'source_CCI3' 提取出 'CCI3'\n",
    "            source_name = entry.name.replace('source_', '')\n",
    "            \n",
    "            # 构造我们感兴趣的高质量数据文件夹的完整路径\n",
    "            high_score_path = os.path.join(entry.path, target_score_dir_name)\n",
    "            \n",
    "            # 3. 检查路径是否存在，然后计算大小\n",
    "            if os.path.exists(high_score_path):\n",
    "                print(f\"正在计算 '{source_name}' 的大小...\")\n",
    "                # 调用我们写的函数来获取文件夹大小\n",
    "                size = get_dir_size(high_score_path)\n",
    "                source_sizes[source_name] = size\n",
    "            else:\n",
    "                # 如果某个source没有这个分数段的数据，也打印出来，方便我们知晓\n",
    "                print(f\"  - 警告: 路径 '{high_score_path}' 不存在，跳过。\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"错误: 根目录 '{root_dir}' 不存在，请检查路径是否正确。\")\n",
    "\n",
    "# 4. 计算总大小和各自的比例\n",
    "total_size = sum(source_sizes.values())\n",
    "\n",
    "print(\"\\n--- 结果分析 ---\")\n",
    "if total_size == 0:\n",
    "    print(\"未能计算任何数据的大小，总大小为 0。请检查目录结构和路径是否正确。\")\n",
    "else:\n",
    "    # 使用我们写的格式化函数，让结果更易读\n",
    "    print(f\"所有 'score_0.75_to_1.0' 数据的总大小: {format_size(total_size)}\\n\")\n",
    "    print(\"各 source 数据大小及其占比:\")\n",
    "    \n",
    "    # 按照大小从大到小排序，这样结果看起来更有条理\n",
    "    sorted_sources = sorted(source_sizes.items(), key=lambda item: item[1], reverse=True)\n",
    "    \n",
    "    for source, size in sorted_sources:\n",
    "        # 计算百分比\n",
    "        proportion = (size / total_size) * 100\n",
    "        # 使用 f-string 格式化输出，让表格对齐，更美观\n",
    "        print(f\"  - {source:<20}: {format_size(size):<10} ({proportion:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取所有高质量数据集（score_0.75_to_1.0）并合并为一个dataset对象\n",
    "from datasets import load_from_disk, concatenate_datasets\n",
    "import os\n",
    "# 定义数据集的根目录和目标分数目录\n",
    "root_dir = '/DATA/disk2/yuhang/.cache/bit_brain_data/chinese-fineweb-classification'\n",
    "target_score_dir_name = 'score_0.75_to_1.0'\n",
    "\n",
    "# 用来存储所有加载的数据集\n",
    "datasets_list = []\n",
    "\n",
    "print(f\"开始加载高质量数据集 (score_0.75_to_1.0)...\")\n",
    "\n",
    "# 遍历根目录，找到所有的 source_* 文件夹\n",
    "try:\n",
    "    for entry in os.scandir(root_dir):\n",
    "        # 确保它是一个目录并且以 'source_' 开头\n",
    "        if entry.is_dir() and entry.name.startswith('source_'):\n",
    "            # 提取 source 的名字\n",
    "            source_name = entry.name.replace('source_', '')\n",
    "            \n",
    "            # 构造高质量数据文件夹的完整路径\n",
    "            high_score_path = os.path.join(entry.path, target_score_dir_name)\n",
    "            \n",
    "            # 检查路径是否存在\n",
    "            if os.path.exists(high_score_path):\n",
    "                print(f\"  正在加载 '{source_name}' 数据集...\")\n",
    "                try:\n",
    "                    # 加载数据集\n",
    "                    source_dataset = load_from_disk(high_score_path)\n",
    "                    # 添加source信息到数据集中，方便后续分析\n",
    "                    #source_dataset = source_dataset.add_column(\"source\", [source_name] * len(source_dataset))\n",
    "                    datasets_list.append(source_dataset)\n",
    "                    print(f\"    成功加载 {len(source_dataset)} 条数据\")\n",
    "                except Exception as e:\n",
    "                    print(f\"    错误: 无法加载 '{source_name}' 数据集: {e}\")\n",
    "            else:\n",
    "                print(f\"  跳过: '{source_name}' - 路径不存在\")\n",
    "\n",
    "    # 合并所有数据集\n",
    "    if datasets_list:\n",
    "        print(f\"\\n正在合并 {len(datasets_list)} 个数据集...\")\n",
    "        dataset = concatenate_datasets(datasets_list)\n",
    "        print(f\"合并完成！总共包含 {len(dataset)} 条高质量数据\")\n",
    "        \n",
    "        # 显示数据集的基本信息\n",
    "        print(f\"\\n数据集信息:\")\n",
    "        print(f\"  - 总样本数: {len(dataset):,}\")\n",
    "        print(f\"  - 列名: {dataset.column_names}\")\n",
    "        \n",
    "    #     # 显示各个source的数据分布\n",
    "    #     if 'source' in dataset.column_names:\n",
    "    #         source_counts = {}\n",
    "    #         for item in dataset:\n",
    "    #             source = item['source']\n",
    "    #             source_counts[source] = source_counts.get(source, 0) + 1\n",
    "            \n",
    "    #         print(f\"\\n各数据源分布:\")\n",
    "    #         for source, count in sorted(source_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    #             percentage = (count / len(dataset)) * 100\n",
    "    #             print(f\"  - {source:<20}: {count:>10,} ({percentage:.2f}%)\")\n",
    "    # else:\n",
    "    #     print(\"错误: 没有找到任何可用的数据集\")\n",
    "    #     dataset = None\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"错误: 根目录 '{root_dir}' 不存在，请检查路径是否正确。\")\n",
    "    dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2TokenizerFast(name_or_path='/home/ytllm/.cache/modelscope/models/Qwen/Qwen3-0.6B', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151665: AddedToken(\"<tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151666: AddedToken(\"</tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151667: AddedToken(\"<think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151668: AddedToken(\"</think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"  \n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\".cache/modelscope/models/Qwen/Qwen3-0.6B\",\n",
    "                                          use_fast=True)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. 定义超参数 ---\n",
    "# max_length 保持不变\n",
    "max_length = 2048\n",
    "# stride (步长) 是新参数。一个常见的选择是 max_length 的 1/4 或 1/2。\n",
    "# 这里我们选择 512，意味着块之间有 2048 - 1024 = 1024 个 token 的重叠。\n",
    "stride = 1024\n",
    "# 新增：定义要保留的最小块长度，小于此长度的最后一个块将被丢弃\n",
    "min_chunk_length = 256\n",
    "\n",
    "# --- 2. 编写新的分词和切块函数 ---\n",
    "def chunk_and_tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    这个函数接收一批文本 (examples)，然后对每个文本进行分词。\n",
    "    如果分词后的长度超过 max_length，就使用滑动窗口将其切分成多个块。\n",
    "    这个函数不仅将长文本切块，还会丢弃掉最后那个过短的块。\n",
    "    \"\"\"\n",
    "    # 首先，对所有文本进行分词，但这次不进行截断和填充，以获取每个文本的完整 token 序列。\n",
    "    # 我们只关心 'input_ids'\n",
    "    full_tokenized = tokenizer(examples['text'], truncation=False, padding=False)\n",
    "    \n",
    "    # 用来存放最终切分好的所有块\n",
    "    chunked_input_ids = []\n",
    "    chunked_attention_mask = []\n",
    "    \n",
    "    # 遍历刚刚分词后的每一篇文章\n",
    "    for input_ids in full_tokenized['input_ids']:\n",
    "        # 获取当前文章的总长度\n",
    "        doc_length = len(input_ids)\n",
    "        \n",
    "        # 如果文章本身就不够长，就直接填充它，作为一个样本\n",
    "        if doc_length <= max_length:\n",
    "            # 填充到 max_length\n",
    "            padded_ids = input_ids + [tokenizer.pad_token_id] * (max_length - doc_length)\n",
    "            chunked_input_ids.append(padded_ids)\n",
    "            # attention_mask 中，真实 token 为 1，填充 token 为 0\n",
    "            attention_mask = [1] * doc_length + [0] * (max_length - doc_length)\n",
    "            chunked_attention_mask.append(attention_mask)\n",
    "        \n",
    "        # 如果文章太长，就开始滑动窗口切分\n",
    "        else:\n",
    "            # 从头开始切\n",
    "            start_index = 0\n",
    "            while start_index < doc_length:\n",
    "                # 定义块的结束位置\n",
    "                end_index = start_index + max_length\n",
    "                \n",
    "                # 从完整序列中切出这个块的 input_ids\n",
    "                chunk = input_ids[start_index:end_index]\n",
    "                \n",
    "                # 如果块的长度小于我们设定的最小阈值，就跳出循环，不再处理这个及之后可能的块\n",
    "                if len(chunk) < min_chunk_length:\n",
    "                    break\n",
    "\n",
    "                # 如果这是最后一个块，且长度不足 max_length，需要填充\n",
    "                if len(chunk) < max_length:\n",
    "                    padded_chunk = chunk + [tokenizer.pad_token_id] * (max_length - len(chunk))\n",
    "                    attention_mask = [1] * len(chunk) + [0] * (max_length - len(chunk))\n",
    "                # 如果是中间的完整块\n",
    "                else:\n",
    "                    padded_chunk = chunk\n",
    "                    attention_mask = [1] * max_length\n",
    "                \n",
    "                chunked_input_ids.append(padded_chunk)\n",
    "                chunked_attention_mask.append(attention_mask)\n",
    "                \n",
    "                # 如果窗口已经滑到了文章末尾，结束循环\n",
    "                if end_index >= doc_length:\n",
    "                    break\n",
    "                    \n",
    "                # 窗口向前滑动一个步长 (stride)\n",
    "                start_index += stride\n",
    "\n",
    "    # 返回一个字典，包含所有新生成的块\n",
    "    return {\n",
    "        'input_ids': chunked_input_ids,\n",
    "        'attention_mask': chunked_attention_mask\n",
    "    }\n",
    "\n",
    "\n",
    "# --- 3. 应用新的函数 ---\n",
    "# 注意：因为一行输入可能产生多行输出，我们必须移除所有原始列，\n",
    "# 否则会因为行数不匹配而报错。\n",
    "tokenized_dataset = dataset.map(\n",
    "    chunk_and_tokenize_function,\n",
    "    batched=True,           # 依然使用批处理以提高效率\n",
    "    num_proc=40,            # 依然使用多进程\n",
    "    remove_columns=dataset.column_names  # 移除所有旧列，非常重要！\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (7/7 shards): 100%|██████████| 254547/254547 [00:02<00:00, 106823.51 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset.save_to_disk(\".cache/pretrain_data/high_score_tokenized_data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
